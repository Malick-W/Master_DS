{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20-NewsGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20NewsGroups fait partie des bases \"textuelles\" célèbres (collection d’environ 20000 articles de journaux).  \n",
    "Le but est alors de les classer de manière automatique par thème. Mon travail s'inspire largement de:  \n",
    "\n",
    "https://github.com/gokriznastic/20-newsgroups_text-classification/blob/master/Multinomial%20Naive%20Bayes-%20BOW%20with%20TF.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.bgu.ac.il/~elhadad/nlp17/Classification_20Groups_Sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extraction d'articles à partir de 20 documents de groupes de discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '/home/malick/Bureau/Data/Statistique_en_grande_dimension/20_newsgroups'\n",
    "\n",
    "# création d'une liste de noms de dossiers pour créer des chemins d'accès valides par la suite\n",
    "folders = [f for f in listdir(my_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer une liste 2D pour stocker la liste de tous les fichiers des différents dossiers\n",
    "\n",
    "files = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    files.append([f for f in listdir(folder_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vérification du nombre total de fichiers rassemblés\n",
    "\n",
    "sum(len(files[i]) for i in range(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'une liste de pathnames de tous les documents\n",
    "#Cela servirait à diviser notre ensemble de données en trains et test\n",
    "\n",
    "pathname_list = []\n",
    "for fo in range(len(folders)):\n",
    "    for fi in files[fo]:\n",
    "        pathname_list.append(join(my_path, join(folders[fo], fi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pathname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un tableau contenant les classes auxquelles appartient chacun des documents\n",
    "\n",
    "Y = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    num_of_files= len(listdir(folder_path))\n",
    "    for i in range(num_of_files):\n",
    "        Y.append(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### splitting the data into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train, doc_test, Y_train, Y_test = train_test_split(pathname_list, Y, random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonctions d'extraction de mots à partir de documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En recherche d'information, un mot vide (ou stopsword, en anglais) est un mot qui est tellement commun qu'il est inutile de l'indexer ou de l'utiliser dans une recherche. En français, des mots vides évidents pourraient être « le », « la », « de », « du », « ce »…\n",
    "\n",
    "Un mot vide est un mot non significatif figurant dans un texte. On l'oppose à mot plein. La signification d'un mot s'évalue à partir de sa distribution (au sens statistique) dans une collection de textes. Un mot est dit « vide » si sa distribution est uniforme sur les textes de la collection. En d'autres termes, un mot qui apparaît avec une fréquence semblable dans chacun des textes de la collection n'est pas discriminant car il ne permet pas de distinguer les textes les uns par rapport aux autres.\n",
    "\n",
    "Lorsque tous les textes de la collection sont rédigés dans une même langue, les mots vides sont principalement des mots caractéristiques de cette langue comme les prépositions, les articles, les pronoms. D'où l'assimilation courante entre mots vides et mots grammaticaux et partant, entre mots pleins et mots lexicaux (noms, verbes, adjectifs). Les listes pré-établies, dites de mots vides, utilisées par les moteurs de recherche sont ainsi des listes de mots grammaticaux. Elles sont parfois appelées « anti-dictionnaires »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    " 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
    " 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    " 'each', 'few', 'for', 'from', 'further', \n",
    " 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
    " 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
    " 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
    " \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    " 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
    " 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
    " 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
    " \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
    " 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    " \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
    " 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
    " 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    " '4th', '5th', '6th', '7th', '8th', '9th', '10th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to preprocess the words list to remove punctuations\n",
    "\n",
    "def preprocess(words):\n",
    "    #we'll make use of python's translate function,that maps one set of characters to another\n",
    "    #we create an empty mapping table, the third argument allows us to list all of the characters \n",
    "    #to remove during the translation process\n",
    "    \n",
    "    #first we will try to filter out some  unnecessary data like tabs\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    words = [word.translate(table) for word in words]\n",
    "    \n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\") \n",
    "    # the character: ' appears in a lot of stopwords and changes meaning of words if removed\n",
    "    #hence it is removed from the list of symbols that are to be discarded from the documents\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in words]\n",
    "    \n",
    "    #some white spaces may be added to the list of words, due to the translate function & nature of our documents\n",
    "    #we remove them below\n",
    "    words = [str for str in stripped_words if str]\n",
    "    \n",
    "    #some words are quoted in the documents & as we have not removed ' to maintain the integrity of some stopwords\n",
    "    #we try to unquote such words below\n",
    "    p_words = []\n",
    "    for word in words:\n",
    "        if (word[0] and word[len(word)-1] == \"'\"):\n",
    "            word = word[1:len(word)-1]\n",
    "        elif(word[0] == \"'\"):\n",
    "            word = word[1:len(word)]\n",
    "        else:\n",
    "            word = word\n",
    "        p_words.append(word)\n",
    "    \n",
    "    words = p_words.copy()\n",
    "        \n",
    "    #we will also remove just-numeric strings as they do not have any significant meaning in text classification\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "    #we will also remove single character strings\n",
    "    words = [word for word in words if not len(word) == 1]\n",
    "    \n",
    "    #after removal of so many characters it may happen that some strings have become blank, we remove those\n",
    "    words = [str for str in words if str]\n",
    "    \n",
    "    #we also normalize the cases of our words\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    #we try to remove words with only 2 characters\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert a sentence into list of words\n",
    "\n",
    "def tokenize_sentence(line):\n",
    "    words = line[0:len(line)-1].strip().split(\" \")\n",
    "    words = preprocess(words)\n",
    "    words = remove_stopwords(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove metadata\n",
    "\n",
    "def remove_metadata(lines):\n",
    "    for i in range(len(lines)):\n",
    "        if(lines[i] == '\\n'):\n",
    "            start = i+1\n",
    "            break\n",
    "    new_lines = lines[start:]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove metadata\n",
    "\n",
    "def remove_metadata(lines):\n",
    "    for i in range(len(lines)):\n",
    "        if(lines[i] == '\\n'):\n",
    "            start = i+1\n",
    "            break\n",
    "    new_lines = lines[start:]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert a document into list of words\n",
    "\n",
    "def tokenize(path):\n",
    "    #load document as a list of lines\n",
    "    f = open(path, 'rb')\n",
    "    text_lines = f.readlines()\n",
    "    \n",
    "    #removing the meta-data at the top of each document\n",
    "    text_lines = remove_metadata(text_lines)\n",
    "    \n",
    "    #initiazing an array to hold all the words in a document\n",
    "    doc_words = []\n",
    "    \n",
    "    #traverse over all the lines and tokenize each one with the help of helper function: tokenize_sentence\n",
    "    for line in text_lines:\n",
    "        doc_words.append(tokenize_sentence(line))\n",
    "\n",
    "    return doc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple helper function to convert a 2D array to 1D, without using numpy\n",
    "\n",
    "def flatten(list):\n",
    "    new_list = []\n",
    "    for i in list:\n",
    "        for j in i:\n",
    "            new_list.append(j)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en utilisant les fonctions ci-dessus sur les données réelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'start' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-57c71272ba5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mlist_of_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-713c66d89642>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#removing the meta-data at the top of each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtext_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#initiazing an array to hold all the words in a document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a79567deaa23>\u001b[0m in \u001b[0;36mremove_metadata\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'start' referenced before assignment"
     ]
    }
   ],
   "source": [
    "list_of_words = []\n",
    "\n",
    "for document in doc_train:\n",
    "        list_of_words.append(flatten(tokenize(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flatten(list_of_words))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "à partir des longueurs ci-dessus, nous observons que le code a été conçu de telle sorte que la liste 2D : list_of_words contient le vocabulaire de chaque fichier document dans chacune de ses lignes, et contient collectivement tous les mots extraits du dossier 20_newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_list_of_words = np.asarray(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the number of unique words that we have extracted from the documents\n",
    "\n",
    "words, counts = np.unique(np_list_of_words, return_counts=True)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the unique words according to their frequency\n",
    "\n",
    "freq, wrds = (list(i) for i in zip(*(sorted(zip(counts, words), reverse=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_o_w = []\n",
    "n_o_w = []\n",
    "for f in sorted(np.unique(freq), reverse=True):\n",
    "    f_o_w.append(f)\n",
    "    n_o_w.append(freq.count(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y = f_o_w\n",
    "x = n_o_w\n",
    "plt.xlim(0,250)\n",
    "plt.xlabel(\"No. of words\")\n",
    "plt.ylabel(\"Freq. of words\")\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we'll start making our train data here onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deciding the no. of words to use as feature\n",
    "\n",
    "n = 5000\n",
    "features = wrds[0:n]\n",
    "print(features[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary that contains each document's vocabulary and ocurence of each word of the vocabulary \n",
    "\n",
    "dictionary = {}\n",
    "doc_num = 1\n",
    "for doc_words in list_of_words:\n",
    "    #print(doc_words)\n",
    "    np_doc_words = np.asarray(doc_words)\n",
    "    w, c = np.unique(np_doc_words, return_counts=True)\n",
    "    dictionary[doc_num] = {}\n",
    "    for i in range(len(w)):\n",
    "        dictionary[doc_num][w[i]] = c[i]\n",
    "    doc_num = doc_num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we make a 2D array having the frequency of each word of our feature set in each individual documents\n",
    "\n",
    "X_train = []\n",
    "for k in dictionary.keys():\n",
    "    row = []\n",
    "    for f in features:\n",
    "        if(f in dictionary[k].keys()):\n",
    "            #if word f is present in the dictionary of the document as a key, its value is copied\n",
    "            #this gives us no. of occurences\n",
    "            row.append(dictionary[k][f]) \n",
    "        else:\n",
    "            #if not present, the no. of occurences is zero\n",
    "            row.append(0)\n",
    "    X_train.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
